"""Collection of models for SHM analysis.
"""

import numpy as np
import numpy.linalg as la #import norm, inv, matrix_rank, pinv
import scipy as sp
import scipy.signal

from . import Tools, Stat

class shfinvsp:
    """Shift invariant space generated by convolution kernels.
    """
    Psi = None  # list of convolution kernels
    Np = None  # all kernels must have the same length
    dim = None  # (Nx-Np+1, Nx) with Nx the size of vectors in the space
    mode = None  # ['valid', 'samel', 'samer', 'full']
    A = None  # convolution matrix
    iA = None  # pseudo inverse of the convolution matrix
    iAA = None  # projection matrix of iA*A
    AiA = None  # projection matrix of iA*A

    def __init__(self, Nx, psi, *args, mode='valid', **kwargs):
        """
        Args:
            Nx (int): dimension of the shift-invariant space.
            psi (1d array): first convolution kernel.
            args (1d arrays): other convolution kernels, optional.
            mode (str): truncation mode of the convolution: ['valid', 'samel', 'samer', 'full'].
        """
        assert(Nx>=len(psi))
        self.Psi = [psi]
        self.Np = len(psi)

        if mode=='valid':
            self.dim = (Nx-self.Np+1, Nx)
        elif mode in ['samel', 'samer']:
            self.dim = (Nx, Nx)
        elif mode=='full':
            self.dim = (Nx+self.Np-1, Nx)
        else:
            raise ValueError('Unknown mode: {}'.format(mode))
        self.mode = mode

        # add the other kernels
        for h in args:
            assert(len(h)==self.Np)
            self.Psi.append(h)

        # super().__init__(**kwargs)

    @staticmethod
    def get_linop(psi, N, mode='valid'):
        """Get the linear operators (see scipy.sparse.linalg.LinearOperator) of a convolution kernel.

        Args:
            psi (1d array): convolution kernel.
            N (int): dimension of the original space.
            mode (str): truncation mode of the convolution.
        Returns:
            A, At: the forward and backward operator.
        """
        assert(len(psi)<=N)

        _matvec = lambda X:Tools.convolve_fwd(X, psi, mode=mode)
    #     _matmat = lambda X:convoleve_fwd(X.T, psi)
        _rmatvec = lambda X:Tools.convolve_bwd(X, psi, mode='full')

        # convolution operator
        dimA = (N-len(psi)+1, N)  # valid mode
        A = sp.sparse.linalg.LinearOperator(dimA, matvec=_matvec, rmatvec=_rmatvec)#, matmat=_matmat)

        # adjoint operator
        dimAt = (N, N-len(psi)+1)  #
        At = sp.sparse.linalg.LinearOperator(dimAt, matvec=_rmatvec, rmatvec=_matvec)#, matmat=_matmat)

        return A, At

    def _get_A(self):
        """Get matrix representation of the convolution operator.
        """
        if self.A is None:
            self.A = Tools.convolve_matrix(self.Psi[0], self.dim[1], mode=self.mode)
            for psi in self.Psi[1:]:
                self.A += Tools.convolve_matrix(psi, self.dim[1], mode=self.mode)
        return self.A

    def _get_iA(self):
        """Get matrix representation of the pseudo inverse of the convolution operator.
        """
        if self.iA is None:  # compute iA when necessary
            self.iA = la.pinv(self._get_A())
        return self.iA

    def analysis(self, X, axis=-1):
        """Analysis or the forward operator.

        Args:
            X (nd array): input array.
            axis (int): axis along which to apply the operator.
        """
        assert(X.shape[axis]==self.dim[1])  # dimension check
        Y = Tools.convolve_fwd(X, self.Psi[0], axis=axis, mode=self.mode)
        for psi in self.Psi[1:]:
            Y += Tools.convolve_fwd(X, psi, axis=axis, mode=self.mode)
        return Y

    def synthesis(self, X, axis=-1):
        """Synthesis or the backward operator (adjoint).
        """
        assert(X.shape[axis]==self.dim[0])
        Y = Tools.convolve_bwd(X, self.Psi[0], axis=axis, mode='full')
        for psi in self.Psi[1:]:
            Y += Tools.convolve_bwd(X, psi, axis=axis, mode='full')
        return Y

    def pinv_analysis(self, X, axis=-1):
        """Pseudo-inverse of the forward operator.
        """
        assert(X.shape[axis]==self.dim[0])
        iA = self._get_iA()
        func = lambda X: np.dot(iA, X)
        return Tools.along_axis(func)(X, axis=axis)

    def pinv_synthesis(self, X, axis=-1):
        """Pseudo-inverse of the backward operator (adjoint).
        """
        assert(X.shape[axis]==self.dim[1])
        iA = self._get_iA()
        func = lambda X: np.dot(iA.T, X)
        return Tools.along_axis(func)(X, axis=axis)

    def projection_Im(self, X, axis=-1):
        """Operator of projection onto the image space generated by Psi (or the orthogonal of the kernel space generated by Psi.T), which is given by Psi * pinv(Psi).
        """
        assert(X.shape[axis]==self.dim[0])
        if self.AiA is None:
            A = self._get_A()
            iA = self._get_iA()
            self.AiA = A @ iA
        func = lambda X: np.dot(self.AiA, X)
        return Tools.along_axis(func)(X, axis=axis)

    def projection_ImT(self, X, axis=-1):
        """Operator of projection onto the image space generated by Psi.T (or the orthogonal of the kernel space generated by Psi), which is given by pinv(Psi) * Psi.
        """
        assert(X.shape[axis]==self.dim[1])
        if self.iAA is None:
            A = self._get_A()
            iA = self._get_iA()
            self.iAA = iA @ A #A.T @ iA.T
        func = lambda X: np.dot(self.iAA, X)
        return Tools.along_axis(func)(X, axis=axis)


class MxDeconv:
    """Multiple inputs vectorial deconvolution.

    The multiple inputs vectorial convolution model is an extention of the classical convolution model: there are k groups of input time series and the convolution kernels are matrices. The model reads:
        Y[t] = \sum_i A_1[i]*X_1[t-i] + ... + \sum_i A_k[i]*X_k[t-i] + C
    where t=0,1,... and
        Y[t] is the responsive time series, it is a n dimensional real vector.
        X_l[t] for l=1...k is the l-th explanary time series, it is a m_l dimensional real vector.
        A_l[i] for l=1...k and i=0...(p_l-1) is the convolution kernel matrix of dimension n-by-m_l.
        C is a n dimensional constant real vector
    The deconvolution problem aims to estimate the kernel matrices A_l[i] and the constant vector C from the time series Y and X_l.
    """
    # dimY = None  # dimension of observation vector
    Nt = None  # length of time series
    Xvar = None  # list of inputs
    Yvar = None  # observation
    lag = None  # list of length of convolution kernel
    _fit_result = None  # result of fit: (L, Cvec, Err, Sig)
    _As = None  # kernel matrices, _As[l][i] is A_l[i]
    _predict_result = None  # result of prediction

    def __init__(self, Y, X, lag, *args):
        """
        Args:
            Y (2d array): observation, each row is a variable, and each column is an observation.
            X (2d array): the first group of input, must have the same duration as y.
            lag (int): length of the convolution kernel corresponded to X.
            *args: more groups of input and lengths of the corresponded convolution kernel, ie, *args=X2, lag2,X3,lag3...
        """
        assert(Y.ndim==2)
        self.Yvar = Y
        self.Nt = self.Yvar.shape[1]

        # add the first group of input
        assert(X.ndim==2 and X.shape[1]==self.Nt)
        assert(lag>=1)
        self.Xvar = [X]
        self.lag = [lag]

        # add the other groups of input
        self.add_inputs(*args)

    def add_inputs(self, *args):
        """Add inputs to the deconvolution model.

        Args:
            X1, lag1, X2, lag2...: input time series and the corresponding kernel length
        """
        for n,V in enumerate(args):
            if n%2==0:
                # All inputs must have the same length as y
                assert(V.ndim==2 and V.shape[1]==self.Nt)
                self.Xvar.append(V)
            else:
                # length of the convolution kernel must be >=1
                assert(V>=1)
                self.lag.append(V)

    @staticmethod
    def _combine_inputs(Xs, lags):
        """Combine the list of inputs and lags consecutively into a single tuple.

        Args:
            Xs (list): list of input time series.
            lags (list): list of kernel length.
        Returns:
            the tuple (Xs[0], lags[0], Xs[1], lags[1]...)
        """
        # assert(len(Xs)==len(lags))
        toto = []
        for X, lag in zip(Xs, lags):
            toto.append(X)
            toto.append(lag)

        return tuple(toto)

    def training_period(self, tidx0=None, Ntrn=None):
        return MxDeconv._training_period(self.Nt, tidx0, Ntrn)

    @staticmethod
    def _training_period(Nt, tidx0=None, Ntrn=None):
        """Compute the valid training period from given parameters.

        Args:
            Nt (int): total length of the data
            tidx0 (int): starting index of the training period.
            Ntrn (int): length of the training period.
        Returns:
            (tidx0, tidx1): tuple of valid starting and ending index.
            Ntrn: length of valid training period.
        """
        tidx0 = 0 if tidx0 is None else (tidx0 % Nt)
        tidx1 = Nt if Ntrn is None else min(tidx0+Ntrn, Nt)
        Ntrn = tidx1 - tidx0

        # tidx1 = 0 if tidx0 is None else min(max(0, tidx0), self.Nt)
        # Ntrn = self.Nt if Ntrn is None else min(max(0, Ntrn), self.Nt)
        # tidx0 = 0 if tidx0 is None else min(max(0, tidx0), self.Nt)
        # # tidx0 = np.max(self.lag)-1
        # tidx1 = tidx0+Ntrn
        return (tidx0, tidx1), Ntrn

    @staticmethod
    def _fit(Yvar, Xvars, lags, constflag, tidx0, tidx1, method):
        """Model fitting (static method).

        Args:
            Yvar (2d array): observation, each row is a variable and each column an observation.
            Xvars (list of 2d array): list of inputs.
            lags (list): length of kernel for each input.
            constflag (bool): if True include the constant vector in the model.
            tidx0 (int): starting index of the training period.
            tidx1 (int): ending index of the training period.
            method (str): method of estimation. ['ls', 'corr', 'yw']: for least-square, correlation and Yule-Walker based method.
        Returns:
            a tuple of results returned by Stat.multi_linear_regression.
        """
        if method=='yw':  # Yule-Walker
            raise NotImplementedError('Yul-Walker estimation is not implemented, use the least-square or correlation method.')
        else:
            Xs = []
            for X, lag in zip(Xvars, lags):
                Xs.append(Tools.mts_cumview(X[:,tidx0:tidx1], lag))  # create the cumulative view from X

            if method=='ls':  # least-square
                _fit_result = Stat.multi_linear_regression_ls(Yvar[:,tidx0:tidx1], *Xs, constflag=constflag)
            elif method=='corr':  # correlation
                _fit_result = Stat.multi_linear_regression_corr(Yvar[:,tidx0:tidx1], *Xs, constflag=constflag)
            else:
                raise ValueError('Unknown method: {}'.format(method))

            return _fit_result

    @staticmethod
    def _predict(Xs, Ls, Cvec=None):
        """Prediction using the trained model.

        The prediction is computed as:
            Ls[0]*Xs[0] + Ls[1]*Xs[1]... + Cvec
        where * denotes the convolution.

        Args:
            Xs (list of 2d array): inputs of explanary variables.
            Ls (list of 2d array): convolution kernels.
            Cvec (1d array): constant vector.
        Returns:
            (Yt, Ys): Yt is the final result and Ys[n] is Ls[n]*Xs[n]
        """
        Ys = []
        for n, X in enumerate(Xs[:len(Ls)]):
            lag = Ls[n].shape[1]//X.shape[0]  # compute the kernel length
            # convolution of the n-th group, recall that Ls[n] is the concatenation of kernel matrices
            toto = np.dot(Ls[n], Tools.mts_cumview(X, lag))
            Ys.append(toto)

        if Cvec is None:
            Yt = np.sum(np.asarray(Ys), axis=0)
        else:
            Yt = np.sum(np.asarray(Ys), axis=0) + Cvec
        return (Yt, Ys)

    def fit(self, constflag=False, tidx0=None, Ntrn=None, method='ls'):
        """Model fitting (wrapper of the static method _fit()).

        Args:
            constflag (bool): if True include the constant vector in the model.
            tidx0 (int): starting index of the training period.
            Ntrn (int): length of the trainng period.
            method (str): method of estimation. ['ls', 'corr', 'yw']: for least-square, correlation and Yule-Walker based method.
        Returns:
            a tuple of results returned by Stat.multi_linear_regression. The results can also be accessed via the variables self._fit_result and self._As.
        """
        (tidx0, tidx1), Ntrn = self.training_period(tidx0, Ntrn)
        self._fit_result = MxDeconv._fit(self.Yvar, self.Xvar, self.lag, constflag, tidx0, tidx1, method)
        self._As = [np.hsplit(self._fit_result[0][n], self.lag[n]) for n in range(len(self.lag))]
        return self._fit_result

    def predict(self, X, *args, constflag=True):
        """Prediction (wrapper of the static method _fit()).

        Args:
            X, *args (2d arrays): the first and other (optional) groups of inputs
            constflag (bool): if False the constant vector will be ignored in prediction, even the model has been trained with constflag=True.
        Return:
            (Yt, Ys): Yt is the final result and Ys is the contribution of each group of inputs. The result is also accesible via the variable self._predict_result.
        Remark:
            *args can be arbitrary. If n groups of inputs are given while k groups presented in the training model, then the effective number of groups taken into consideration in prediction is min(n,k).
        """
        if self._fit_result is None:
            raise ValueError('Fit the model first!')

        Ls, Cvec, Err, Sig2 = self._fit_result
        if constflag:
            self._predict_result = MxDeconv._predict([X]+list(args), Ls, Cvec)
        else:
            self._predict_result = MxDeconv._predict([X]+list(args), Ls)
        return self._predict_result


class OmxDeconv(MxDeconv):
    """Ordered multiple inputs deconvolution.

    Unlike MxDeconv where all inputs are fitted simultaneously, the ordered deconvolution fits the inputs by order:
        Y is first fitted with X1,
        then the residual is fitted with X2, etc.
    """
    def fit(self, constflag=False, tidx0=None, Ntrn=None, method='ls'):
        (tidx0, tidx1), Ntrn = self.training_period(tidx0, Ntrn)

        self._fit_result = []
        self._As = []
        Err = self.Yvar

        for X, lag in zip(self.Xvar, self.lag):
            res = MxDeconv._fit(Err, [X], [lag], constflag, tidx0, tidx1, method)
            L, Cvec, Err, Sig2 = res
            self._fit_result.append(res)
            self._As.append(L[0])

        return self._fit_result

    def predict(self, X, *args, constflag=True):
        Ys = []
        for n, V in enumerate([X]+list(args)):
            L, Cvec, Err, Sig2 = self._fit_result[n]
            if constflag:
                toto, _ = MxDeconv._predict([V], L, Cvec=Cvec)
            else:
                toto, _ = MxDeconv._predict([V], L)
            Ys.append(toto)
        Yt = np.sum(np.asarray(Ys), axis=0)

        self._predict_result = (Yt, Ys)
        return self._predict_result


class DmxDeconv(MxDeconv):
    """Decorrelated multiple inputs deconvolution.

    Unlike MxDeconv where all inputs are fitted simultaneously, the decorrelated deconvolution first transforms the inputs into decorrelated time series:
        X1 -> W1,
        X2 is first fitted with X1, the residual -> W2,
        X3 is first fitted with X1, and X2, the residual -> W3, etc.
    Then Y is fitted with W1, W2...
    For prediction the inputs are decorrelated using the models trained by the procedure of decorrelation above, then the results are feeded to the model trained by the fitting of Y.
    """
    Mxds = None  # list of MxDeconv instances for decorrelation (decorrelation models)
    dconstflag = None  # use constant in the fitting of decorrelation models
    # _decorr_result = None

    def __init__(self, Y, X, lag, *args, dconstflag=False, dlen=None):
        """
        Args:
            dconstflag (bool):
        """
        # raw inputs, remark that Xvar here will denote the decorrelated inputs
        Xraw = [X] + [V for V in args[::2]]
        self.dconstflag = dconstflag

        # lags for decorrelation models
        dlags = [lag]+list(args[1::2]) if dlen is None else [dlen]*len(Xraw)

        # Decorrelation of inputs. Mxds is the trained decorrelation models, and Ws is the decorrelated inputs which will be stored as self.Xvar. We drop the last lag in dlags since it is not used in the decorrelation models.
        self.Mxds, Ws = DmxDeconv._decorr_fit(Xraw, dlags[:-1], constflag=dconstflag)

        # Call parent class constructor with the decorrelated inputs
        toto = self._combine_inputs(Ws[1:], args[1::2])
        super().__init__(Y, X, lag, *toto)

    @staticmethod
    def _decorr_fit(Xs, lags, constflag=True):
        """Fit the decorrelation models for the multiple inputs X_1, X_2,...X_n.

        Args:
            Xs (list of 2d array): inputs.
            lags (list of int): kernel length of each decorrelation model.
            constflag (bool): if True add constant vector in the decorrelation models.
        Returns:
            Mxds: list of decorrelation models (objects of MxDeconv class).
            Ws: decorrelated inputs.
        """
        assert(len(lags)>=len(Xs)-1)  #
        Mxds = [None]  # by convention the first model is empty
        Ws = [Xs[0].copy()]  # and no decorrelation for the first group of input

        for n in range(len(Xs)-1):
            args = MxDeconv._combine_inputs(Xs[1:n+1], lags[1:n+1])  # Xs[-1] is never used
            toto = MxDeconv(Xs[n+1], Xs[0], lags[0], *args)
            res = toto.fit(constflag=constflag)
            Ws.append(res[2])  # res[2] is the residual
            Mxds.append(toto)
        return Mxds, Ws

    @staticmethod
    def _decorr_predict(Mxds, X, *args, constflag=True):
        """Decorrelation of inputs using trained models.

        Args:
            Mxds (list): list of fitted MxDeconv objects.
            X (2d array): first group of input.
            args (2d arrays): more groups of input.
            constflag (bool): if False the constant vector will be ignored in the prediction, even if the decorrelation modes are trained with the constant.
        Returns:
            a list of decorrelated inputs.
        """
        Ws = [X]
        for n in range(min(len(Mxds)-1,len(args))):
            # By convention, Mxds[0] is None
            res = Mxds[n+1].predict(X, *args[:n], constflag=constflag)
            Ws.append(args[n] - res[0])
        return Ws

    def decorr(self, X, *args, constflag=True):
        """Decorrelation of inputs using trained models (wrapper of _decorr_predict()).
        """
        return DmxDeconv._decorr_predict(self.Mxds, X, *args, constflag=constflag)

    def predict(self, X, *args, constflag=True):
        # decorrelation of the inputs
        Ws = self.decorr(X, *args, constflag=self.dconstflag)
        return super().predict(Ws[0], *Ws[1:], constflag=constflag)

        # Ls, Cvec, Err, Sig2 = self._fit_result
        #
        # if constflag:
        #     self._predict_result = MxDeconv._predict(Ws, Ls, Cvec)
        # else:
        #     self._predict_result = MxDeconv._predict(Ws, Ls)
        # return self._predict_result


class ShfDeconv(shfinvsp, DmxDeconv):
    """Multiple inputs deconvolution based on shift invariant space.
    """

    povec = None  # pseudo-inverse of the constant vector 1
    _predict_result_final = None  # an additional variable to keep final results

    def __init__(self, Psi, Y, X, lag, *args, **kwargs):
        """
        Args:
            Psi (list): convolution kernels of the shift-invariant space.
            other arguments: see the constructor of DmxDeconv.
        """
        shfinvsp.__init__(self, Y.shape[1], Psi[0], *Psi[1:], mode='valid')  # constructor of shfinvsp
        Ycof = self.analysis(Y)
        Xcof = [self.analysis(x) for x in [X]+list(args[::2])]
        toto = self._combine_inputs(Xcof[1:], args[1::2])
        DmxDeconv.__init__(self, Ycof, Xcof[0], lag, *toto, **kwargs)
        self.povec = self.pinv_analysis(np.ones((1, self.dim[0])), axis=-1)

    def predict(self, X, *args, constflag=True):
        """
        Args:
            X, *args: same as in super().predict()
            constflag (bool): if False the constant vectors of deconvolution and decorrelation models will be ignored even if these models are trained with these constant vectors.
        Returns:
            (Yt, Ys): Yt is the final result and Ys is the contribution of each group of inputs. The result is also accesible via the variable self._predict_result_final.
        """
        # # Not adding constant vector in the two following steps
        # Ws = self.decorr(X, *args, constflag=False)
        # Am = MxDeconv.predict(Ws[0], *Ws[1:], constflag=False)
        # Not adding constant vector here
        At, As = super().predict(X, *args, constflag=False)

        # Compute the constant vector
        bv = self._fit_result[1]
        for n in range(1, len(self.lag)):
            Smat = np.sum(self._As[n], axis=0)  # sum of the kernel matrices in the n-th group
            cvec = self.Mxds[n]._fit_result[1]  # constant vectors of decorrelation model
            bv -= np.dot(Smat, cvec)

        # Final result
        if constflag:
            Yt = self.projection_ImT(At) + np.dot(bv, self.povec)
        else:
            Yt = self.projection_ImT(At)
        Ys = [self.projection_ImT(y) for y in As]
        self._predict_result_final = (Yt, Ys)

        return self._predict_result_final

class DiffDeconv(ShfDeconv):
    """Multiple inputs deconvolution based on shift invariant space generated by a differential kernel.

    The differential kernel psi is the auto-convolution of [1,-1]: at degree 0 it is [1,-1], at degree 1 it is [1, -2, 1] etc. The list of kernels Psi here is [psi], and Ker(Psi), or the kernel space generated by Psi, is just the linear space of polynomials up to the degree of psi. We use the numpy routine for fast computation of the projection operators.
    """
    deg = None  # degree of the differential kernel

    def __init__(self, Y, X, lag, *args, **kwargs):
        """
        Args:
            Keyword arguments:
            deg (int): degree of the differential kernel.
        """
        if 'deg' in kwargs:
            self.deg = kwargs['deg']
            kwargs.pop('deg')
        else:
            self.deg = 0
        psi = Tools.diff_kernel(self.deg, step=1)  # create the differential kernel
        super().__init__([psi], Y, X, lag, *args, **kwargs)  # call the constructor of ShfDeconv

    def pinv_analysis(self, X, axis=-1):
        if self.mode=='full':  # no simple analytical formula
            return super().pinv_analysis(X, axis=axis)
        else:
            assert(X.shape[axis]==self.dim[0])
            if self.mode=='valid':
                slc = [slice(None)]*X.ndim
                slc[axis] = slice(1,None)
                def mycumsum(X, axis):
                    dimX = list(X.shape); dimX[axis]+=1
                    toto = np.zeros(dimX)
                    toto[slc] = np.cumsum(X, axis=axis)
                    return toto

                for n in range(self.deg+1):
                    X = mycumsum(X, axis=axis)
                return self.projection_ImT(X, axis=axis)
                ## Although theoretically exact, the numerical precision of the following is worse:
                # for n in range(self.deg+1):
                #     ss = diffinvsp(X.shape[axis]+1, self.deg, mode='valid')
                #     X = ss.projection_ImT(mycumsum(X, axis=axis))
                # return X
            elif self.mode=='samel':
                return Tools.rfunc(self.deg+1, np.cumsum, X, axis=axis)
            else:  # self.mode=='samer':
                slc = [slice(None)]*X.ndim
                slc[axis] = slice(None,None,-1)
                mycumsum = lambda x,axis: -1*np.cumsum(x[slc], axis=axis)[slc]
                return Tools.rfunc(self.deg+1, mycumsum, X, axis=axis)

    def pinv_synthesis(self, X, axis=-1):
        if self.mode=='full':  # no simple analytical formula
            return super().pinv_analysis(X, axis=axis)
        else:
            assert(X.shape[axis]==self.dim[1])
            if self.mode=='valid':  # Not working
                slc0 = [slice(None)]*X.ndim; slc1=slc0.copy()
                slc0[axis] = slice(-1,0,-1)
                slc1[axis] = slice(None,None,-1)
                mycumsum = lambda x,axis: np.cumsum(X[slc0], axis=axis)[slc1]

                X = self.projection_ImT(X, axis=axis)
                for n in range(self.deg+1):
                    X = mycumsum(X, axis=axis)
                return X
            elif self.mode=='samel':
                slc = [slice(None)]*X.ndim
                slc[axis] = slice(None,None,-1)
                mycumsum = lambda x,axis: np.cumsum(x[slc], axis=axis)[slc]
                return Tools.rfunc(self.deg+1, mycumsum, X, axis=axis)
            else:  # self.mode=='samer':
                mycumsum = lambda x,axis: -1*np.cumsum(x, axis=axis)
                return Tools.rfunc(self.deg+1, mycumsum, X, axis=axis)

    def projection_Im(self, X, axis=-1):
        assert(X.shape[axis]==self.dim[0])
        if self.mode in ['full']:
            return X - Tools.polyprojection(X, deg=self.deg, axis=axis)
        else:
            return X

    def projection_ImT(self, X, axis=-1):
        assert(X.shape[axis]==self.dim[1])
        if self.mode in ['valid']:
            return X - Tools.polyprojection(X, deg=self.deg, axis=axis)
        else:
            return X
