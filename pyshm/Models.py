"""Collection of models for SHM analysis.
"""

import numpy as np
import numpy.linalg as la
from numpy import ma
import scipy.linalg
import numbers

from . import Tools, Stat, Kalman
# from .Kalman import Kalman


class shfinvsp:
    """Shift invariant space generated by convolution kernels.
    """
    Psi = None  # list of convolution kernels
    Np = None  # all kernels must have the same length
    dim = None  # (Nx-Np+1, Nx) with Nx the size of vectors in the space
    mode = None  # ['valid', 'samel', 'samer', 'full']
    A = None  # convolution matrix
    iA = None  # pseudo inverse of the convolution matrix
    iAA = None  # projection matrix of iA*A
    AiA = None  # projection matrix of iA*A

    def __init__(self, Nx, psi, *args, mode='valid'):
        """
        Args:
            Nx (int): dimension of the shift-invariant space.
            psi (1d array): first convolution kernel.
            args (1d arrays): other convolution kernels, optional.
            mode (str): truncation mode of the convolution: ['valid', 'samel', 'samer', 'full'].
        """
        assert Nx >= len(psi)
        self.Psi = [psi]
        self.Np = len(psi)

        if mode == 'valid':
            self.dim = (Nx-self.Np+1, Nx)
        elif mode in ['samel', 'samer']:
            self.dim = (Nx, Nx)
        elif mode == 'full':
            self.dim = (Nx+self.Np-1, Nx)
        else:
            raise ValueError('Unknown mode: {}'.format(mode))
        self.mode = mode

        # add the other kernels
        for h in args:
            assert len(h) == self.Np
            self.Psi.append(h)

        # super().__init__(**kwargs)

    @staticmethod
    def get_linop(psi, N, mode='valid'):
        """Get linear operators associated to the convolution kernel.

        The linear operator here is an object of scipy.sparse.linalg.LinearOperator.

        Args:
            psi (1d array): convolution kernel.
            N (int): dimension of the original space.
            mode (str): truncation mode of the convolution.

        Returns:
            A, At: the forward and backward operator.
        """
        assert len(psi) <= N

        _matvec = lambda X: Tools.convolve_fwd(X, psi, mode=mode)
        _rmatvec = lambda X: Tools.convolve_bwd(X, psi, mode='full')

        # convolution operator
        dimA = (N-len(psi)+1, N)  # valid mode
        A = sp.sparse.linalg.LinearOperator(dimA, matvec=_matvec, rmatvec=_rmatvec)#, matmat=_matmat)

        # adjoint operator
        dimAt = (N, N-len(psi)+1)
        At = sp.sparse.linalg.LinearOperator(dimAt, matvec=_rmatvec, rmatvec=_matvec)#, matmat=_matmat)

        return A, At

    def _get_A(self):
        """Get matrix representation of the convolution operator.
        """
        if self.A is None:
            self.A = Tools.convolve_matrix(self.Psi[0], self.dim[1], mode=self.mode)
            for psi in self.Psi[1:]:
                self.A += Tools.convolve_matrix(psi, self.dim[1], mode=self.mode)
        return self.A

    def _get_iA(self):
        """Get matrix representation of the pseudo inverse of the convolution operator.
        """
        if self.iA is None:  # compute iA when necessary
            self.iA = la.pinv(self._get_A())
        return self.iA

    def analysis(self, X, axis=-1):
        """Analysis or the forward operator.

        Args:
            X (nd array): input array.
            axis (int): axis along which to apply the operator.
        """
        assert X.shape[axis]==self.dim[1]  # dimension check
        Y = Tools.convolve_fwd(X, self.Psi[0], axis=axis, mode=self.mode)
        for psi in self.Psi[1:]:
            Y += Tools.convolve_fwd(X, psi, axis=axis, mode=self.mode)
        return Y

    def synthesis(self, X, axis=-1):
        """Synthesis or the backward operator (adjoint).
        """
        assert X.shape[axis]==self.dim[0]
        Y = Tools.convolve_bwd(X, self.Psi[0], axis=axis, mode='full')
        for psi in self.Psi[1:]:
            Y += Tools.convolve_bwd(X, psi, axis=axis, mode='full')
        return Y

    def pinv_analysis(self, X, axis=-1):
        """Pseudo-inverse of the forward operator.
        """
        assert X.shape[axis]==self.dim[0]
        iA = self._get_iA()
        func = lambda X: np.dot(iA, X)
        return Tools.along_axis(func)(X, axis=axis)

    def pinv_synthesis(self, X, axis=-1):
        """Pseudo-inverse of the backward operator (adjoint).
        """
        assert X.shape[axis]==self.dim[1]
        iA = self._get_iA()
        func = lambda X: np.dot(iA.T, X)
        return Tools.along_axis(func)(X, axis=axis)

    def projection_Im(self, X, axis=-1):
        """Operator of projection onto the image space of Psi (or the orthogonal of the kernel space of Psi.T)

        This projector is given by Psi * pinv(Psi).
        """
        assert(X.shape[axis]==self.dim[0])
        if self.AiA is None:
            A = self._get_A()
            iA = self._get_iA()
            self.AiA = A @ iA
        func = lambda X: np.dot(self.AiA, X)
        return Tools.along_axis(func)(X, axis=axis)

    def projection_ImT(self, X, axis=-1):
        """Operator of projection onto the image space of Psi.T (or the orthogonal of the kernel space of Psi)

        This projector is given by pinv(Psi) * Psi.
        """
        assert(X.shape[axis]==self.dim[1])
        if self.iAA is None:
            A = self._get_A()
            iA = self._get_iA()
            self.iAA = iA @ A #A.T @ iA.T
        func = lambda X: np.dot(self.iAA, X)
        return Tools.along_axis(func)(X, axis=axis)


class MxDeconv:
    """Multiple inputs vectorial deconvolution.

    The multiple inputs vectorial convolution model is an extention of the classical convolution model: there are k groups of input time series and the convolution kernels are matrices. The model reads:
        Y[t] = \sum_i A_1[i]*X_1[t-i] + ... + \sum_i A_k[i]*X_k[t-i] + C
    where t=0,1,... and
        Y[t] is the responsive time series, it is a n dimensional real vector.
        X_l[t] for l=1...k is the l-th explanary time series, it is a m_l dimensional real vector.
        A_l[i] for l=1...k and i=0...(p_l-1) is the convolution kernel matrix of dimension n-by-m_l.
        C is a n dimensional constant real vector
    The deconvolution problem aims to estimate the kernel matrices A_l[i] and the constant vector C from the time series Y and X_l.
    """
    # dimY = None  # dimension of observation vector
    Nt = None  # length of time series
    Xvar = None  # list of inputs
    Yvar = None  # observation
    lag = None  # list of length of convolution kernel
    _fit_result = None  # result of fit: (L, Cvec, Err, Sig)
    _As = None  # kernel matrices, _As[l][i] is A_l[i]
    _predict_result = None  # result of prediction
    # _split_dim = None

    def __init__(self, Y, X, lag, *args):
        """
        Args:
            Y (2d array): observation, each row is a variable, and each column is an observation.
            X (2d array): the first group of input, must have the same duration as y.
            lag (int): length of the convolution kernel corresponded to X.
            *args: more groups of input and lengths of the corresponded convolution kernel, ie, *args=X2, lag2,X3,lag3...
        """
        assert(Y.ndim==2)
        self.Yvar = Y
        self.Nt = self.Yvar.shape[1]

        # add the first group of input
        assert(X.ndim==2 and X.shape[1]==self.Nt)
        assert(lag>=1)
        self.Xvar = [X]
        self.lag = [lag]

        # add the other groups of input
        self.add_inputs(*args)

        # dimension of split
        self._split_dim = []
        k = 0
        for n in range(len(self.Xvar)):
            k += self.lag[n] * self.Xvar[n].shape[0]
            self._split_dim.append(k)

    def add_inputs(self, *args):
        """Add inputs to the deconvolution model.

        Args:
            X1, lag1, X2, lag2...: input time series and the corresponding kernel length
        """
        for n, V in enumerate(args):
            if n % 2 == 0:
                # All inputs must have the same length as y
                assert(V.ndim == 2 and V.shape[1] == self.Nt)
                self.Xvar.append(V)
            else:
                # length of the convolution kernel must be >=1
                assert(V >= 1)
                self.lag.append(V)

    @staticmethod
    def _combine_inputs(Xs, lags):
        """Combine the list of inputs and lags consecutively into a single tuple.

        Args:
            Xs (list): list of input time series.
            lags (list): list of kernel length.
        Returns:
            the tuple (Xs[0], lags[0], Xs[1], lags[1]...)
        """
        # assert(len(Xs)==len(lags))
        toto = []
        for X, lag in zip(Xs, lags):
            toto.append(X)
            toto.append(lag)

        return tuple(toto)

    def training_period(self, tidx0=None, Ntrn=None):
        return Stat.training_period(self.Nt, tidx0, Ntrn)

    @staticmethod
    def _cumview(Xvars, lags):
        Xs = []
        for x, lag in zip(Xvars, lags):
            Xs.append(Tools.mts_cumview(x, lag))  # create the cumulative view from X
        return np.vstack(Xs)

    def fit(self, sidx=0, Ntrn=None, dspl=1, snr2=None, clen2=None, vthresh=0., corrflag=False, Nexp=0, method="mean"):
        """Model fitting.

        Args:
            sidx (int): starting index of the training period.
            Ntrn (int): length of the trainng period.
            method (str):
        Returns:
            a tuple of results returned by Stat.multi_linear_regression. The results can also be accessed via the variables self._fit_result and self._As.
        """
        Xs = MxDeconv._cumview(self.Xvar, self.lag)
        _, self._fit_result = Stat.deconv(self.Yvar, Xs, 1, dord=0, sidx=sidx, Ntrn=Ntrn, dspl=dspl, snr2=snr2, vthresh=vthresh, corrflag=corrflag, Nexp=Nexp, method=method)
        return self._fit_result

    def predict(self):
        """Prediction.

        Args:
            X, *args (2d arrays): the first and other (optional) groups of inputs
            constflag (bool): if False the constant vector will be ignored in prediction, even the model has been trained with constflag=True.
        Return:
            (Yt, Ys): Yt is the final result and Ys is the contribution of each group of inputs. The result is also accesible via the variable self._predict_result.
        Remark:
            *args can be arbitrary. If n groups of inputs are given while k groups presented in the training model, then the effective number of groups taken into consideration in prediction is min(n,k).
        """
        if self._fit_result is None:
            raise ValueError('Fit the model first!')

        Xs = MxDeconv._cumview(self.Xvar, self.lag)
        Amat, Cvec, *_ = self._fit_result
        Yprd = Amat @ Xs
        Err = self.Yvar - (Amat @ Xvar + Cvec)  # residual
        Sig = Stat.cov(Err, Err)  # covariance matrix
        self._predict_result = (Yprd, Err, Sig)
        return Yprd


class DiffDeconv(MxDeconv):
    dord = 1  # differential order

    # def __init__(self, Y, X, lag, *args):
    #     super().__init__(Y, X, lag, *args)

    def fit(self, sidx=0, Ntrn=None, dspl=1, snr2=None, clen2=None, vthresh=0., corrflag=False, Nexp=0, method="mean"):
        Xs = MxDeconv._cumview(self.Xvar, self.lag)
        _, self._fit_result = Stat.diffdeconv(self.Yvar, Xs, 1, sidx=sidx, Ntrn=Ntrn, dspl=dspl, snr2=snr2,vthresh=vthresh, corrflag=corrflag, Nexp=Nexp, method=method)
        return self._fit_result

    def predict(self):
        if self._fit_result is None:
            raise ValueError('Fit the model first!')

        Xs = MxDeconv._cumview(self.Xvar, self.lag)
        Amat, Cvec, *_ = self._fit_result
        toto = Amat @ Xs
        Yprd = toto - Tools.polyprojection(toto, deg=self.dord-1, axis=-1)  # projection \Psi^\dagger \Psi
        Err = self.Yvar - (Amat @ Xs + Cvec)  # residual
        Sig = Stat.cov(Err, Err)  # covariance matrix
        self._predict_result = (Yprd, Err, Sig)
        return Yprd


class DiffDeconvBM(MxDeconv):
    _Bs = None    # obs. matrices of each group
    _constflag = None  # use of constant trend in the model
    _Kalman = None  # Kalman filter object
    _dimobs = None  # dimension of the observation vector
    _dimsys = None  # dimension of the system state vector
    _Ks = None  # sequence of kernel matrices, which are the state vector in KF
    _Ka = None  # same as Ks, except that the kernel matrices of the same group are not splitted

    def fit(sigmaq2, sigmar2, x0=0., p0=1., constflag=False):
        """Initialize the Kalman filter.

        Args:
            sigmaq2 (float): transition noise level (variance)
            sigmar2 (float): observation noise level (variance)
            x0 (float): initial guess of the system state vector (a constant vector)
            p0 (float): initial guess of the covariance matrix (a diagonal matrix)
            constflag (bool): add constant trend in the model
        Returns:
            a Kalman filter object.
        """
        self._dimobs, Nt = self.Yvar.shape  # dimension of the observation vector and duration

        # construct the observation matrices: time-dependent
        self._dimsys = 0
        self._Bs = []

        for X, lag in zip(self.Xvar, self.lag):
            Xc = Tools.mts_cumview(X, lag)
            B0 = np.zeros((Nt, self._dimobs, Xc.shape[0] * self._dimobs))
            self._dimsys += B0.shape[-1]

            for t in range(Nt):
                toto = Xc[:,t].copy()
                toto[np.isnan(toto)] = 0
                B0[t,] = np.kron(np.eye(self._dimobs), toto)
            self._Bs.append(B0)

        self._dimsys += self._dimobs if constflag else 0

        B = np.zeros((Nt, self._dimobs, self._dimsys))
        for t in range(Nt):
            if constflag:
                B[t,] = np.hstack([b[t] for b in self._Bs]+np.eye(self._dimobs))
            else:
                B[t,] = np.hstack([b[t] for b in self._Bs])

        self._constflag = constflag

        # construct the transition matrix: time-independent
        A = np.eye(self._dimsys)

        self._Kalman = Kalman.Kalman(self.Yvar.T, A, B, G=None, Q=sigmaq2, R=sigmar2, X0=x0, P0=p0)  # initialize the kalman filter
        return self._Kalman

    def predict(self, smooth=False):
        """Kalman filtration or smoothing (for compatibility reasons it is called 'predict' here).

        Args:
            smooth (bool): if True use kalman smoother otherwise use kalman filter.
        Returns:
            (Yt, Ys, Cvec): A tuple with the following members:
                Yt (3d array): filtered/smoothed observation
                Ys (list): contribution of each group of exogeneous inputs
                Cvec (3d array): constant trend
            Ks: sequence of kernel matrices
            Ka: same as Ks, except that the kernel matrices of the same group are not splitted
        """
        if self._Kalman.init_state is None or self._Kalman.init_covariance is None or self._Kalman.transition_covariance is None or self._Kalman.observation_covariance is None:
            raise ValueError('Incorrect parameters: run fit() first.')
            return

        if smooth:
            LXtn, LPtn, LJt, res = self._Kalman.smoother()
            Xflt = np.transpose(np.asarray(LXtn), (0,2,1))
        else:
            LXtt, LPtt, LXtm, LPtm, LEt, LSt, LKt, LmXt, *_ = self._Kalman.filter()
            Xflt = np.transpose(np.asarray(LXtt), (0,2,1))

        # analysis of results
        Yflt = np.sum(self._Kalman.observation_matrices * Xflt, axis=-1).T  # filtered/smoothed observations, note self.Yvar is not the original observation but its derivative. Take transpose to make the second axis as time axis.

        self._Ks = []  # sequence of kernel matrices, note that the kernel matrices are the state vector in KF
        self._Ka = []  # same as Ks, except that the kernel matrices of the same group are not splitted
        Cvec = []
        for t in range(self.Yvar.shape[1]):
            V = np.reshape(Xflt[t,], (self._dimobs, -1))  # state vector reshaped as a matrix
            if self._constflag:
                Cvec.append(np.atleast_2d(V[:,-1]))
                S = np.split(V[:,:-1], self._split_dim[:-1], axis=-1)  # split into groups
            else:
                Cvec.append(np.atleast_2d(np.zeros(self._dimobs)))
                S = np.split(V, self._split_dim[:-1], axis=-1)  # split into groups
            self._Ka.append(S)
            toto = []
            for n,lag in enumerate(self.lag):
                toto.append(np.split(S[n], lag, axis=-1))
            self._Ks.append(toto)
        Cvec = np.vstack(Cvec).T

        self._predict_result = (Yflt, self.Yvar-Yflt, self._Ka, self._Ks)  # filtered observation (indeed the derivative), residual, kernel matrices, full result

        Yt = self.pinv_analysis(Yflt)
        # Yt = self.projection_ImT(self.pinv_analysis(Yflt))
        Ys = []

        for n in range(len(self.lag)):
            X = np.vstack([k[n] for k in self._Ka])
            toto = np.sum(self._Bs[n] * X[:,np.newaxis,:], axis=-1).T
            Ys.append(self.pinv_analysis(toto)) ##
            # Ys.append(self.projection_ImT(self.pinv_analysis(toto)))

        self._predict_result_final = (Yt, Ys, Cvec, Yflt)

        return self._predict_result_final, self._Ks, self._Ka
